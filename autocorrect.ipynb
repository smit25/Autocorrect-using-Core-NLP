{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autocorrect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smit25/Autocorrect-using-Core-NLP/blob/main/autocorrect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqS_BJLE-B7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b559e6d0-df16-4e35-a65d-09573ab4d839"
      },
      "source": [
        "# AUTOCORRECT\n",
        "\"\"\"\n",
        "https://repository.lib.fit.edu/bitstream/handle/11141/682/ZHANG-THESIS.pdf?isAllowed=y&sequence=1\n",
        "https://arxiv.org/abs/1706.07786\n",
        "http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36180.pdf\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "# ASSUMPTIONS\n",
        "\"\"\"\n",
        "- The first word in the input sentence is correct\n",
        "- The dataset is not exhaustive\n",
        "- (without bias) If a given word exists in the dictionary, it is assumed to be correct\n",
        "- (with bias) the context of the sentence is checked for with more weightage to bigram and trigram for autocorrecting a word in input already existing in the dictionary\n",
        "- punctuation is not considered\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nhttps://repository.lib.fit.edu/bitstream/handle/11141/682/ZHANG-THESIS.pdf?isAllowed=y&sequence=1\\nhttps://arxiv.org/abs/1706.07786\\n \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbTcsOz91Oeu"
      },
      "source": [
        "!apt install enchant\r\n",
        "!pip install pyenchant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmphkgrIAMzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6c6d86-8883-4fb7-852d-47da386985aa"
      },
      "source": [
        "import re\r\n",
        "import random\r\n",
        "from collections import Counter\r\n",
        "import pandas as pd\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "import string\r\n",
        "import enchant\r\n",
        "import tensorflow as tf\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "from nltk.tag import pos_tag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HcqU2CP-HzY"
      },
      "source": [
        "ins = 1\r\n",
        "delete = 1\r\n",
        "sub = 1\r\n",
        "trans = 1\r\n",
        "threshold = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Iomyn_D--OZ"
      },
      "source": [
        "# LOAD WORDS\r\n",
        "def words(text):\r\n",
        "  return re.findall(r'\\w+', text.lower())\r\n",
        "\r\n",
        "word_list = Counter(words(open('wordlist.txt').read()))\r\n",
        "n =sum (word_list.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WTNkEA5BDoY"
      },
      "source": [
        "# LEVENSHTEIN EDIT DISTANCE CRUDE ALGO\r\n",
        "def edit_dis_algo(s1, s2):\r\n",
        "  l = [1001]*(len(s2)+1)\r\n",
        "  dp = [l]*(len(s1)+1)\r\n",
        " \r\n",
        "  for j in range(len(s2)+1):\r\n",
        "    dp[0][j] = j\r\n",
        "  for i in range(len(s1)+1):\r\n",
        "    dp[i][0] = i\r\n",
        "\r\n",
        "  \r\n",
        "  for i in range(len(s1)+1):\r\n",
        "    for j in range(len(s2)+1):\r\n",
        "      if i==0 or j==0:\r\n",
        "        continue\r\n",
        "      if s1[i-1] == s2[j-1]:\r\n",
        "        dp[i][j] = dp[i-1][j-1]\r\n",
        "      else:\r\n",
        "        dp[i][j] = min(dp[i-1][j-1] + rep, dp[i-1][j] + delete, dp[i][j-1] + ins)\r\n",
        "        if i>2 and j>2 and s1[i-1] == s2[j-2] and s1[i-2] == s1[j-1]:\r\n",
        "          dp[i][j] = min(dp[i][j], dp[i-3][j-3] + trans)\r\n",
        "  \r\n",
        "  return dp[len(s1)][len(s2)]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiBynao_GLZl"
      },
      "source": [
        "# LEVENSHTEIN - EDIT DISTANCE FOR THE LM\r\n",
        "def edit(s):\r\n",
        "  edited_list = set()\r\n",
        "  alpha = 'abcdefghijklmnopqrstuvwxyz'\r\n",
        "  l = set()\r\n",
        "  print(s)\r\n",
        "  l.add(s)\r\n",
        "  for iter in range(threshold):\r\n",
        "    for word in l:\r\n",
        "      split_word = [(word[:i], word[i:])   for i in range(len(word) + 1)]\r\n",
        "      deletes    = [i + j[1:]               for i, j in split_word if j]\r\n",
        "      transposes = [i + j[1] + j[0] + j[2:] for i, j in split_word if len(j)>1]\r\n",
        "      replaces   = [i + c + j[1:]           for i, j in split_word if j for c in alpha]\r\n",
        "      inserts    = [i + c + j               for i, j in split_word for c in alpha]\r\n",
        "  \r\n",
        "      l = set(deletes + transposes + replaces + inserts)\r\n",
        "      #print(l)\r\n",
        "\r\n",
        "    for ss in l:\r\n",
        "      if d.check(ss):\r\n",
        "        edited_list.add(ss)\r\n",
        "  return edited_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggwj4AZgYZbr"
      },
      "source": [
        "############## EDIT DISTANCE  ###############"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjdA8mtgIid8",
        "outputId": "016e611f-da46-4d56-d8c4-376d66520cea"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwQ1-OyVIl4"
      },
      "source": [
        "USING THE IMDB DATASET\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idY4ylDS8YpH"
      },
      "source": [
        "# IMDB DATASET\r\n",
        "#def IMDB_to_csv(directory):    \r\n",
        "#   data = pd.DataFrame()\r\n",
        "    \r\n",
        "#   for filename in glob.glob(str(directory)+'/neg/*.txt'):\r\n",
        "#     with open(filename, 'r',  encoding=\"utf8\") as f:\r\n",
        "#       content = f.readlines()\r\n",
        "#       content_table = pd.DataFrame({'id':filename.split('_')[0].split('/')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'neg', 'text':content})\r\n",
        "#       data = data.append(content_table)\r\n",
        "        \r\n",
        "#   for filename in glob.glob(str(directory)+'/pos/*.txt'):\r\n",
        "#     with open(filename, 'r',  encoding=\"utf8\") as f:\r\n",
        "#       content = f.readlines()\r\n",
        "#       content_table = pd.DataFrame({'id':filename.split('_')[0].split('/')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'pos', 'text':content})\r\n",
        "#     data = data.append(content_table)\r\n",
        "#     data = data.sort_values(['pol','id'])\r\n",
        "#     data = data.reset_index(drop=True)\r\n",
        "#     #data['rating_norm'] = (data['rating'] - data['rating'].min())/( data['rating'].max() - data['rating'].min() )\r\n",
        "\r\n",
        "#   return(data)\r\n",
        "\r\n",
        "# dir ='/content/drive/MyDrive/aclImdb_v1.tar.gz'\r\n",
        "# imdb_data = IMDB_to_csv(dir)\r\n",
        "# print(imdb_data)\r\n",
        "# imdb_data.columns = ['id', 'dataset', 'text', 'pol','file']\r\n",
        "# imdb_data.head()\r\n",
        "# data1 = imdb_data['text'].split('.')\r\n",
        "# train_data_sent = pd.DataFrame()\r\n",
        "\r\n",
        "# for index in imdb_data.index:\r\n",
        "#     data_row = imdb_data.iloc[index,:]\r\n",
        "\r\n",
        "#     for sent_id in range(0,len(data_row['text'].split('.'))-1):\r\n",
        "#         sentence = data_row['text'].split('.')[sent_id]\r\n",
        "#         sentence_row = pd.DataFrame({\r\n",
        "#                                      'id':data_row['id'],\r\n",
        "#                                      'pol':data_row['pol'],\r\n",
        "#                                      'sent_id':sent_id,\r\n",
        "#                                      'sentence':sentence}, index = [index]) \r\n",
        "#         train_data_sent = train_data_sent.append(sentence_row)\r\n",
        "\r\n",
        "# train_data_sent.head()\r\n",
        "# train_data_sent['sentence_clean'] = train_data_sent['sentence'].str.replace('[{}]'.format(string.punctuation), '')\r\n",
        "# train_data_sent['sentence_clean'] = train_data_sent['sentence_clean'].str.lower()\r\n",
        "\r\n",
        "# train_data_sent['sentence_clean'] = '<s> ' + train_data_sent['sentence_clean']\r\n",
        "# train_data_sent['sentence_clean'] = train_data_sent['sentence_clean'] + ' </s>'\r\n",
        "\r\n",
        "# train_data_sent.head()\r\n",
        "# text = train_data_sent['sentence_clean']\r\n",
        "# text_list = \" \".join(map(str, text))\r\n",
        "# print(text_list[:200])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D17c6RTbRtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b8c81f-f73f-4400-dfdf-6a253f3040ee"
      },
      "source": [
        "# LOAD DATASET AND DICT\r\n",
        "d = enchant.Dict(\"en_US\") \r\n",
        "dataset = (open('wordlist.txt').read())\r\n",
        "print(dataset[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
            "by Sir Arthur Conan Doyle\n",
            "(#15 in o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwC15nLldgHL"
      },
      "source": [
        "#PREPROCESS THE DATA\r\n",
        "def preprocess(data):\r\n",
        "  temp = data.replace('[{}]'.format(string.punctuation), '')\r\n",
        "  temp_list = temp.split('.')\r\n",
        "  for i,sent in enumerate(temp_list):\r\n",
        "    words = sent.split(' ')\r\n",
        "    for j in range(1,len(words)):\r\n",
        "      if words[j]!='' and words[j][0].isupper():\r\n",
        "        words[j] = '<unk>'\r\n",
        "    temp_list[i] = \" \".join(words).lower()\r\n",
        "\r\n",
        "  temp_list[:] = ['<s>' + s + '</s>' for s in temp_list]\r\n",
        "  return temp_list\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prp-sgcCx2-1"
      },
      "source": [
        "# PREPROCESSING THE INPUT\r\n",
        "def input_process(inp):\r\n",
        "  inp = re.sub(r'[^\\w\\s]','',inp)\r\n",
        "  temp_list = inp.split(' ')\r\n",
        "  v = len(Counter(temp_list))\r\n",
        "  wordlist = []\r\n",
        "  wordlist = ['<s>'] + temp_list + ['</s>']\r\n",
        "  return wordlist, v\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ryRJIHjHCE"
      },
      "source": [
        "# FUNCTIONS FOR N-GRAM WITH STUPID BACKOFF/ LAPLACIAN SMOOTHING\r\n",
        "backoff = 0.4\r\n",
        "alpha = 1.5\r\n",
        "smoothing = {'backoff':1, 'laplacian':2}\r\n",
        "\r\n",
        "def unigram(st,case, v):\r\n",
        "  prob =  len(re.findall(' ' + st + ' ', data))\r\n",
        "  if case == 1:\r\n",
        "    return prob/n\r\n",
        "  if case == 2:\r\n",
        "    return (prob + alpha)/n + v*alpha\r\n",
        "\r\n",
        "def bigram(st1, st2, case, v):\r\n",
        "  len1 = len(re.findall(' ' + st1 + ' '+ st2 + ' ', data))\r\n",
        "  #print(st1,'llllll')\r\n",
        "  len2 = len(re.findall(' ' + st1 + ' ', data))\r\n",
        "  if len2 == 0:\r\n",
        "    return 0\r\n",
        "  prob = len1/len2\r\n",
        "  #print('bigram: ', prob)\r\n",
        "  if case == 1:\r\n",
        "    return prob if prob >0 else backoff*unigram(st2, case, v)\r\n",
        "  if case == 2:\r\n",
        "    return (len1 + alpha/len2 + v*alpha)\r\n",
        "\r\n",
        "def trigram(st1, st2, st3, case , v):\r\n",
        "  len1 = len(re.findall(' '+ st1 + ' ' + st2 + ' ' + st3+ ' ',data))\r\n",
        "  len2 = len(re.findall(' ' + st1 + ' '+ st2 +' ', data))\r\n",
        "  if len2 == 0:\r\n",
        "    return np.square(backoff)*unigram(st3, case, v)\r\n",
        "  prob = len1/len2\r\n",
        "  #print('trigram: ', prob)\r\n",
        "  if case == 1:\r\n",
        "    return prob if prob>0 else  backoff*bigram(st2, st3,case ,v)\r\n",
        "  if case == 2:\r\n",
        "    return (len1 + alpha/ len2 + v*alpha)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNndJ5rn2j1"
      },
      "source": [
        "# TRAIN INTERPOLATION COEFFICIENTS\r\n",
        "x1 = tf.Variable(initial_value=0, name='x1', trainable=True, dtype=tf.float32)\r\n",
        "x2 = tf.Variable(initial_value=0, name='x2', trainable=True, dtype=tf.float32)\r\n",
        "\r\n",
        "def loss(sent):\r\n",
        "  return (perplexity(sent,x1,x2,1-x1-x2) - 1)** 2\r\n",
        "  \r\n",
        "def train(train_data):\r\n",
        "  x1.assign(0.33)\r\n",
        "  x2.assign(0.33)\r\n",
        "  opt = tf.keras.optimizers.Adadelta(learning_rate = 0.01)\r\n",
        "\r\n",
        "  for sent in train_data:\r\n",
        "    with tf.GradientTape(persistent=False) as tape:\r\n",
        "      loss_fn = loss(sent)\r\n",
        "    vars = [x1, x2]\r\n",
        "    grads = tape.gradient(loss_fn, vars)\r\n",
        "    opt.apply_gradients(zip(grads, vars))\r\n",
        "\r\n",
        "    # ERROR\r\n",
        "    # err = np.abs(x.numpy() - x_init)\r\n",
        "    # print(\"Iteration: %d Val: %f Error: %f Grad: %f\" % (\r\n",
        "    #         opt.iterations.numpy(), \r\n",
        "    #         x.numpy(), \r\n",
        "    #         err, \r\n",
        "    #         grads[0].numpy()))\r\n",
        "\r\n",
        "    print('hi')\r\n",
        "  return x1, x2, 1-x1-x2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQTe_-ON7oB5"
      },
      "source": [
        "# INTERPOLATION TRAIN FUNCTION FOR 3 WORDS\n",
        "def interp_train(str_list, x1,x2,x3,v = 3):\n",
        "  case = smoothing['backoff']\n",
        "  prob = x1*unigram(str_list[2], case, v) + x2*bigram(str_list[2], str_list[1], case, v) + x3*trigram(str_list[2], str_list[1], str_list[0], case, v)\n",
        "  print('prob:', prob)\n",
        "  return prob\n",
        "\n",
        "# FUNCTION TO CALCULATE PERPLEXITY AND CROSS ENTROPY OF LM\n",
        "def perplexity(text, x1, x2, x3):\n",
        "  words = text.split(' ')\n",
        "  words= list(filter(lambda x: x!= ' ' or x!='', words))\n",
        "  words = ['<s>'] + words + ['</s>']\n",
        "  n, v = len(words), len(Counter(words))\n",
        "  score = 0\n",
        "  entr =0\n",
        "  for i in range(2,len(words)):\n",
        "    str_list = [words[i-2], words[i-1], words[i]]\n",
        "    val = interp_train(str_list, x1, x2, x3 , v)\n",
        "    score += val\n",
        "    #entr += val*np.log(val)\n",
        "  #score = np.exp(score)*100000\n",
        "  print('score:',score)\n",
        "  return score*10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_H388IXFFA"
      },
      "source": [
        "# FUNCTION TO GENERATE TRAINING DATASET\r\n",
        "def generate_train_data(data_len, word_len):\r\n",
        "  train_data = list()\r\n",
        "\r\n",
        "  for i in range(data_len):\r\n",
        "    length = 0\r\n",
        "    l = len(clean_list)\r\n",
        "    while length<4:\r\n",
        "      ind = random.randint(0,l-1)\r\n",
        "      sent_list = clean_list[ind].split(' ')\r\n",
        "      length = len(sent_list)\r\n",
        "\r\n",
        "    word_ind = random.randint(0, len(sent_list)-3)\r\n",
        "    temp_list = []\r\n",
        "    for j in range(word_ind, word_ind + word_len):\r\n",
        "      temp_list.append(sent_list[j])\r\n",
        "    train_str = \" \".join(temp_list)\r\n",
        "    train_data.append(train_str)\r\n",
        "  print(train_data)\r\n",
        "  return train_data\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfN7asn7udas"
      },
      "source": [
        "mock_params = [[0.4, 0.6, 0.0], [0.15, 0.35, 0.5], [0.25, 0.35, 0.4]]\n",
        "cases = len(mock_params)\n",
        " \n",
        "# APPLYING INTERPOLATION\n",
        "def interpolation(*args, **kwargs):\n",
        "  c,v = kwargs['c'], kwargs['v']\n",
        "  x1,x2,x3 = params[c-1]\n",
        "  str_list =[arg for arg in args]\n",
        "  case = smoothing['backoff']\n",
        "  prob = x1*unigram(str_list[2], case, v) + x2*bigram(str_list[2], str_list[1], case, v) + x3*trigram(str_list[2], str_list[1], str_list[0], case, v)\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2xnbRSXo93"
      },
      "source": [
        "# N-GRAM FUNCTION FOR CALLING INTERPOLATION FOR DIFFERENT CASES\n",
        "def ngram(str1, str2, str3, v):\n",
        "  if d.check(str3):\n",
        "    return str3\n",
        "  possible_words = list(edit(str3))\n",
        "  prob = 0\n",
        "  ans = -1\n",
        "  pred_word = ''\n",
        "  # ASSUMPTION IS THAT FIRST WORD IF EXISTS IN THE DICTIONARY IS CORRECT\n",
        "  for word in possible_words:\n",
        "    if str1 == ' ' and not d.check(str3):\n",
        "      prob = interpolation(str1, str2, word, c =1, v = v)\n",
        "  # elif str1 != ' ' and d.check(str3):\n",
        "  #   prob = interpolation(str1, str2, word, c =2, v=v)\n",
        "    elif str1 != ' ' and not d.check(str3):\n",
        "      prob = interpolation(str1, str2, word, c =3, v=v)\n",
        "    if prob > ans:\n",
        "      ans = prob\n",
        "      pred_word = word\n",
        "    \n",
        "  return pred_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnnrwUPNNzc7"
      },
      "source": [
        "# SLIDING WINDOW ALGORITHM\r\n",
        "def autocorrect(inp_list,sentence,v):\r\n",
        "  corrected_list = []\r\n",
        "  postag = pos_tag(sentence.split())\r\n",
        "  prev2, prev1, cur = '', '', '<s>'\r\n",
        "  for i in range(1,len(inp_list)-1):\r\n",
        "    word = inp_list[i]\r\n",
        "    #print(word, '----')\r\n",
        "    if i<len(inp_list)-1 and postag[i-1][1] == 'NNP':\r\n",
        "      corrected_list.append(word)\r\n",
        "      cur = '<unk>'\r\n",
        "      continue\r\n",
        "    # if not word_list[word] and d.check(word):\r\n",
        "    #   word = '<unk>'\r\n",
        "    if i<2:\r\n",
        "      prev1 = cur\r\n",
        "      cur = word\r\n",
        "      prev2 = ''\r\n",
        "    else:\r\n",
        "      if i==2:\r\n",
        "        prev2 = prev1\r\n",
        "      else:\r\n",
        "        prev2 = corrected_list[i-2-1]\r\n",
        "\r\n",
        "      prev1 = corrected_list[i-1-1]\r\n",
        "      cur = word\r\n",
        "    # print(prev2, '11')\r\n",
        "    # print(prev1, '22')\r\n",
        "    # print(cur, '33')\r\n",
        "    new_word = ngram(prev2, prev1, cur, v)\r\n",
        "    print(new_word, ' new-word')\r\n",
        "    if new_word != ' ':\r\n",
        "      corrected_list.append(new_word)\r\n",
        "\r\n",
        "  return \" \".join(corrected_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03PN9PByezP2"
      },
      "source": [
        "clean_list = preprocess(dataset)\r\n",
        "data = \" \".join(map(str, clean_list))\r\n",
        "print(data[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r01KvytWV_ue"
      },
      "source": [
        "# TRAIN INTERPOLATION PARAMETERS\r\n",
        "train_data = generate_train_data(3, 3)\r\n",
        "x1, x2, x3 = train(train_data)\r\n",
        "x1_new = x1.numpy()\r\n",
        "x2_new = x2.numpy()\r\n",
        "x3_new = x3.numpy()\r\n",
        "params = [x1_new, x2_new, x2_new]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "u8oVEOcnDVZJ",
        "outputId": "eee3d677-3ec9-42c3-e94d-0de0ae2c5a83"
      },
      "source": [
        "# MAIN\r\n",
        "input_str = 'How aee yuu'\r\n",
        "input_list,v = input_process(input_str)\r\n",
        "#print(input_list)\r\n",
        "output = autocorrect(input_list, input_str, v)\r\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How  new-word\n",
            "aee\n",
            "are  new-word\n",
            "yuu\n",
            "you  new-word\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How are you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWqhHOYdH_7p"
      },
      "source": [
        "d.check('arr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fySNWJy5DJCL"
      },
      "source": [
        "#### FLASK CODE ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8sc3gAzDMFA"
      },
      "source": [
        "!pip install flask-ngrok\r\n",
        "!pip install flask\r\n",
        "%mkdir templates -p\r\n",
        "%mkdir static/css -p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1Hf_rtmJwBm",
        "outputId": "fa91c87c-5be6-4654-ff88-1b316b37de5f"
      },
      "source": [
        "## HTML CODE\r\n",
        "%%writefile templates/index.html\r\n",
        "<!DOCTYPE html>\r\n",
        "<html>\r\n",
        "<head>\r\n",
        "<meta charset=\"UTF-8\"> \r\n",
        "<title>AUTOCORRECT</title>\r\n",
        "<link href= 'style.css' rel='stylesheet' type='text/css'>\r\n",
        "</head>\r\n",
        "<body>\r\n",
        "<div class=\"text-input\">\r\n",
        "<form method = \"POST\" action = \"{{url_for('index')}}\">\r\n",
        "  <input type=\"text\" id=\"input1\" name = \"input\" placeholder=\"Try typing something in here!\">\r\n",
        "  <label for=\"input1\">Your input</label>\r\n",
        "  <input type = \"submit\" id = \"submit\" value = \"Go!\">\r\n",
        "</form>\r\n",
        "</div>\r\n",
        "<div class=\"text-input\">\r\n",
        "  <input type=\"text\" id=\"output1\" placeholder=\"Your output will be shown here!\" value =\"{{ output_str }}\">\r\n",
        "</div>\r\n",
        "</body>\r\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting templates/index.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVDXav3sLfiv",
        "outputId": "94e7443a-3e87-4ad4-f957-2bce26291e51"
      },
      "source": [
        "## CSS CODE\r\n",
        "%%writefile static/style.css\r\n",
        "@import  url(https://fonts.googleapis.com/css?family=Montserrat);\r\n",
        "\r\n",
        "body{\r\n",
        "  display: flex;\r\n",
        "  flex-direction: column;\r\n",
        "  align-items: center;\r\n",
        "  justify-content: center;\r\n",
        "  height: 100vh;\r\n",
        "  font-family: Montserrat;\r\n",
        "  background: #313E50;\r\n",
        "}\r\n",
        "\r\n",
        ".text-input{\r\n",
        "  \r\n",
        "  position: relative;\r\n",
        "  margin-top: 50px;\r\n",
        "  \r\n",
        "  input[type=\"text\"]{\r\n",
        "    display: inline-block;\r\n",
        "    width: 500px;\r\n",
        "    height: 40px;\r\n",
        "    box-sizing: border-box;\r\n",
        "    outline: none;\r\n",
        "    border: 1px solid lightgray;\r\n",
        "    border-radius: 3px;\r\n",
        "    padding: 10px 10px 10px 100px;\r\n",
        "    transition: all 0.1s ease-out;\r\n",
        "  }\r\n",
        "  \r\n",
        "  input[type=\"text\"] + label{\r\n",
        "    position: absolute;\r\n",
        "    top: 0;\r\n",
        "    left: 0;\r\n",
        "    bottom: 0;\r\n",
        "    height: 40px;\r\n",
        "    line-height: 40px;\r\n",
        "    color: white;\r\n",
        "    border-radius: 3px 0 0 3px;\r\n",
        "    padding: 0 20px;\r\n",
        "    background: #E03616;\r\n",
        "    transform: translateZ(0) translateX(0);\r\n",
        "    transition: all 0.3s ease-in;\r\n",
        "    transition-delay: 0.2s;\r\n",
        "  }\r\n",
        "  \r\n",
        "  input[type=\"text\"]:focus + label{\r\n",
        "    transform: translateY(-120%) translateX(0%);\r\n",
        "    border-radius: 3px;\r\n",
        "    transition: all 0.1s ease-out;\r\n",
        "  }\r\n",
        "  \r\n",
        "  input[type=\"text\"]:focus{\r\n",
        "    padding: 10px;\r\n",
        "    transition: all 0.3s ease-out;\r\n",
        "    transition-delay: 0.2s;\r\n",
        "  }\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting static/style.css\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzh5U6uVD_Xq"
      },
      "source": [
        "## FLASK BACKEND CODE\r\n",
        "from flask_ngrok import run_with_ngrok\r\n",
        "from flask import Flask, render_template, redirect, url_for, request\r\n",
        "\r\n",
        "app = Flask(__name__, static_folder = 'static')\r\n",
        "run_with_ngrok(app) \r\n",
        "\r\n",
        "@app.route('/', methods = ['GET','POST'])\r\n",
        "def index():\r\n",
        "  output_str = 'TEST'\r\n",
        "  if request.method == 'POST':\r\n",
        "    input = request.form['input']\r\n",
        "    input_list,v = input_process(input)\r\n",
        "    output = autocorrect(input_list, input, v)\r\n",
        "    output_str = output\r\n",
        "\r\n",
        "  return render_template('index.html', output_str = output_str)\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "  app.run()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elnj5gNHSCo3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}